---
# This config file contains variables for the experiment that are datset 
  # independent (i.e. not specific to any dataset), Such as directory paths, 
  # or quantity of synthetic datasets to generate

# Directory paths for the folders used in the experiment (relative to the 'notebooks' directory)
folders:    
    log_dir:       "../logs/"              # for logging data for each specific dataset

    data_dir:      "../data/"              # datasets
    meta_dir:      "../data/metadata/"     
    result_dir:    "../data/result/"     
    raw_dir:       "../data/raw/"             # datasets
    sd_dir:        "../data/synthetic/"       # synthetic dataset
    real_dir:      "../data/real/"            # the original datasets, real data
    train_dir:     "../data/train/"           # train data for models built on original data
    test_dir:      "../data/test/"            # the hold-out data for testing from the original/real datset

    pickles_dir:   "../pickles/"           # for saved pickle objects:
    models_dir:    "../pickles/models/"      # contains the saved models as pickles
    SDGs_dir:      "../pickles/SDGs/"        # contains the trained SDG for each setting
    settings_dir:  "../pickles/settings/"    # contains the dataset specific settings for the experiments
    setup_dir:  "../pickles/pycaret_setup/"  # contains the dataset specific pycaret settings for the experiments

    model_perf_filepath: "../data/result/model_performance.csv"

clf:    # Settings for classification modelling
    cv_folds: 10 # number of folds used in k-fold cross validation
    # choice of machine learning models to compare natively in pycaret compare_models(), using mostly sklearn
    ml_models: [
                'lr',         # Logistic Regression
                'knn',        # K-Nearest Neighbor
                'rbfsvm',     # SVM - radial basis function kernel
                'rf',         # Random Forest
                ### All models above used in experiment
                #'nb',         # Naive Bayes #issue with cuml
                #'mlp',        # Multilayer Perceptron
                #'gbc',        # Gradient Boosting Classifier
                #'svm',        #NOTICE: PYCARET BUGG, do not use, said to be Support Vector Machines but returns SDGClassifier model
                ### Not implmented Tuning_Grids ###
                # 'ridge',    # Ridge Classifier TODO: some error shows up when ridge gets choosen
                #'gpc',        # Gaussian Process Classifier
                #'dt',         # Decision Tree Classifier
                #'et',         # Extra Trees Classifier
                #'qda',        # Quadratic Discriminant Analysis
                #'ada',        # Ada Boost Clasifier
                #'lda',        # Linear Dicriminant Analysis
                #'xgboost',    # Extreme Gradient Boosting
                #'lightgbm'    #Light Gradient Boosting Machine
                ]   
    # The default tuning grids of pycaret can be found in https://github.com/pycaret/pycaret/blob/master/pycaret/containers/models/classification.py
    tuning_param:  
        #n_iter: 100             # (do not use when early_stopping is used) Number of iterations to run the tune_model, i.e. n_iter paramter
        optimize: 'Accuracy'   # (dont change! cuml only have currently accuracy) The target metric to optimize against
        choose_better: True
        search_library: 'optuna'
        search_algorithm: 'tpe'
        early_stopping: 'Median'  # median stopping rule
        early_stopping_max_iters: 1000 # Default=10, maximum of different candidates to try


# CTGAN hyperparameters
ctgan_param:
    # Possible hyperparameters for the model:
    #    embedding_dim (int):
    #            Size of the random sample passed to the Generator. Defaults to 128.
    #    generator_dim (tuple or list of ints):
    #        Size of the output samples for each one of the Residuals. A Residual Layer
    #        will be created for each one of the values provided. Defaults to (256, 256).
    #    discriminator_dim (tuple or list of ints):
    #        Size of the output samples for each one of the Discriminator Layers. A Linear Layer
    #        will be created for each one of the values provided. Defaults to (256, 256).
    #    generator_lr (float):
    #        Learning rate for the generator. Defaults to 2e-4.
    #    generator_decay (float):
    #        Generator weight decay for the Adam Optimizer. Defaults to 1e-6.
    #    discriminator_lr (float):
    #        Learning rate for the discriminator. Defaults to 2e-4.
    #    discriminator_decay (float):
    #        Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.
    #    batch_size (int):
    #        Number of data samples to process in each step.
    #    discriminator_steps (int):
    #        Number of discriminator updates to do for each generator update.
    #        From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper
    #        default is 5. Default used is 1 to match original CTGAN implementation.
    #    log_frequency (boolean):
    #        Whether to use log frequency of categorical levels in conditional
    #        sampling. Defaults to ``True``.
    #    verbose (boolean):
    #        Whether to have print statements for progress results. Defaults to ``False``.
    #    epochs (int):
    #        Number of training epochs. Defaults to 300.
    #    pac (int):
    #        Number of samples to group together when applying the discriminator.
    #        Defaults to 10.
    #    cuda (bool or str):
    #        If ``True``, use CUDA. If a ``str``, use the indicated device.
    #        If ``False``, do not use cuda at all.
    #    learn_rounding_scheme (bool):
    #        Define rounding scheme for ``FloatFormatter``. If ``True``, the data returned by
    #        ``reverse_transform`` will be rounded to that place. Defaults to ``True``.
    #    enforce_min_max_values (bool):
    #        Specify whether or not to clip the data returned by ``reverse_transform`` of
    #        the numerical transformer, ``FloatFormatter``, to the min and max values seen
    #        during ``fit``. Defaults to ``True``.
    quality_params:     # list of parameters set to generate synthetic data of varying qualities
        #Q0:
        #  epochs: 3
        #  batch_size: 100
        #  pac: 10
        #  discriminator_steps: 1
        #  generator_lr: 0.05000
        #  discriminator_lr: 0.05000
        #  verbose: True #required (for capturing loss values)
        #  cuda: True
        #Q1: # TODO: to be added or ignore, and remake above to Q1
        #  epochs: 30
        #  batch_size: 250
        #  pac: 10
        #  discriminator_steps: 2
        #  generator_lr: 0.00500
        #  discriminator_lr: 0.00500
        #  verbose: True #required (for capturing loss values)
        #  cuda: True
        #Q2:
        #  epochs: 300
        #  batch_size: 500
        #  pac: 10
        #  discriminator_steps: 5 
        #  generator_lr: 0.00200
        #  discriminator_lr: 0.00200
        #  verbose: True #required (for capturing loss values)
        #  cuda: True
        Q3:
          epochs: 800
          batch_size: 500
          pac: 10
          verbose: True #required (for capturing loss values)
          discriminator_steps: 8 
          generator_lr: 0.00002
          discriminator_lr: 0.00002
          cuda: True

    # how many samples to generate by factor the sample size of the original dataset
    sd_size_factor: 1   
    num_sd: 10  # 10 how many copies of SDG and synthetic datasets to create for each setting 

# Specifiy which datasets to run the experiment on, 
#   if null (in yaml null == None in python), run all
#   if ['dataset id1', 'dataset id2'] , runs experiment on all datasets with the specified id
#   in yaml, list is done:
#                       - item1
#                       - item2
#                       - item3
run_dataset: 
  - 'D0'    #Step: 2, 3(q0,1,2,3), working on 4# diabetes (~750 samples)
  - 'D1'    #Step: 2, 3(q0,1,2,3), working on 4# titanic  (~800 samples)
  - 'D205' #Step: 2, 3(q0,1,2,3), working on 4# bank      (5% of 45 211 samples = 2260 )
  - 'D305' #Step: 2, 3(q0,1,2 working on q3),  # mnist     (5% of 70 000 samples = 3500)
  #- 'D2'   #broken  # bank     (45 211 samples)
  #- 'D3'   #brodken # mnist    (70 000 samples)
...
