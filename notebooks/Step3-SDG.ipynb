{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f9411b-734f-4616-aab0-a36163b7fcab",
   "metadata": {},
   "source": [
    "# Step 3: SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563deb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\flore\\\\source\\\\repos\\\\master-thesis-vt23\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81739df5-f21f-4ee5-8109-992bc7c8519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for the section\n",
    "from sdv.single_table import CTGANSynthesizer as CTGAN\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import os \n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import (getPicklesFromDir, \n",
    "                   getExperimentConfig, \n",
    "                   extract_loss_info_from_stdout, \n",
    "                   create_loss_plot)\n",
    "\n",
    "from mlflow_manager import MLFlowManager\n",
    "\n",
    "# Get global experiment settings\n",
    "config = getExperimentConfig()\n",
    "# Get folders\n",
    "folders = config['folders']\n",
    "# Get dataset specific settings\n",
    "dataset_settings = getPicklesFromDir(folders['settings_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8070cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def capture_stdout(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Save the original stdout\n",
    "        original_stdout = sys.stdout\n",
    "\n",
    "        # Create a new StringIO object to temporarily redirect stdout\n",
    "        sys.stdout = StringIO()\n",
    "        \n",
    "        # Call the original function and get its output\n",
    "        func_output = func(*args, **kwargs)\n",
    "\n",
    "        # Retrieve the captured stdout\n",
    "        captured_stdout = sys.stdout.getvalue()\n",
    "        \n",
    "        # Close IO and restore the original stdout\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "        # Return both the function output and the captured stdout\n",
    "        return func_output, captured_stdout\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@capture_stdout\n",
    "def train_sdg_model(model, data, sdg_name):\n",
    "    print(\"#START#\")\n",
    "    print(sdg_name)\n",
    "    model.fit(data)\n",
    "    print(\"#END#\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9a99e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Start: SDG-D2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#START#\n",
      "SD2Q1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.61 GiB for an array with shape (45211, 45211) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\ctgan\\data_transformer.py\", line 129, in _transform_discrete\n    return ohe.transform(data).to_numpy()\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\rdt\\transformers\\base.py\", line 52, in wrapper\n    return function(self, *args, **kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\rdt\\transformers\\base.py\", line 369, in transform\n    data = self._add_columns_to_data(data, transformed_data, self.output_columns)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\rdt\\transformers\\base.py\", line 263, in _add_columns_to_data\n    data = pd.concat([data, transformed_data.set_index(data.index)], axis=1)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\core\\frame.py\", line 6017, in set_index\n    frame = self.copy()\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\core\\generic.py\", line 6368, in copy\n    data = self._mgr.copy(deep=deep)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 649, in copy\n    res = self.apply(\"copy\", deep=deep)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 352, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"C:\\Users\\flore\\miniconda3\\envs\\syn\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 549, in copy\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 7.61 GiB for an array with shape (45211, 45211) and data type int32\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m CTGAN(metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquality_params[quality])\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#print(sdg_name)   # for capturing loss info\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#model.fit(original_data)\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m model, stdout_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sdg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# extract loss, create loss plot and save it\u001b[39;00m\n\u001b[0;32m     54\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m extract_loss_info_from_stdout(stdout_loss)\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mtrain_sdg_model\u001b[1;34m(model, data, sdg_name)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#START#\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(sdg_name)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#END#\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\sdv\\single_table\\base.py:457\u001b[0m, in \u001b[0;36mBaseSynthesizer.fit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    456\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(data)\n\u001b[1;32m--> 457\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_processed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\sdv\\single_table\\base.py:441\u001b[0m, in \u001b[0;36mBaseSynthesizer.fit_processed_data\u001b[1;34m(self, processed_data)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_processed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, processed_data):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit this model to the transformed data.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m        processed_data (pandas.DataFrame):\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m            The transformed data used to fit the model to.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fitted_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\sdv\\single_table\\ctgan.py:114\u001b[0m, in \u001b[0;36mCTGANSynthesizer._fit\u001b[1;34m(self, processed_data)\u001b[0m\n\u001b[0;32m    112\u001b[0m discrete_columns \u001b[38;5;241m=\u001b[39m detect_discrete_columns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metadata(), processed_data)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m CTGAN(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscrete_columns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\ctgan\\synthesizers\\base.py:50\u001b[0m, in \u001b[0;36mrandom_state.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m set_random_states(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_random_state):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\ctgan\\synthesizers\\ctgan.py:308\u001b[0m, in \u001b[0;36mCTGAN.fit\u001b[1;34m(self, train_data, discrete_columns, epochs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer \u001b[38;5;241m=\u001b[39m DataTransformer()\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39mfit(train_data, discrete_columns)\n\u001b[1;32m--> 308\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_sampler \u001b[38;5;241m=\u001b[39m DataSampler(\n\u001b[0;32m    311\u001b[0m     train_data,\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39moutput_info_list,\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_frequency)\n\u001b[0;32m    315\u001b[0m data_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39moutput_dimensions\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\ctgan\\data_transformer.py:179\u001b[0m, in \u001b[0;36mDataTransformer.transform\u001b[1;34m(self, raw_data)\u001b[0m\n\u001b[0;32m    174\u001b[0m     column_data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronous_transform(\n\u001b[0;32m    175\u001b[0m         raw_data,\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_transform_info_list\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     column_data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_column_transform_info_list\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(column_data_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\ctgan\\data_transformer.py:163\u001b[0m, in \u001b[0;36mDataTransformer._parallel_transform\u001b[1;34m(self, raw_data, column_transform_info_list)\u001b[0m\n\u001b[0;32m    160\u001b[0m         process \u001b[38;5;241m=\u001b[39m delayed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_discrete)(column_transform_info, data)\n\u001b[0;32m    161\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(process)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocesses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\syn\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.61 GiB for an array with shape (45211, 45211) and data type int32"
     ]
    }
   ],
   "source": [
    "# Specify datasets by Id, if None, all is run\n",
    "run_dataset = config['run_dataset']\n",
    "\n",
    "# get settings\n",
    "quality_params = config['ctgan_param']['quality_params']\n",
    "sd_size_factor = config['ctgan_param']['sd_size_factor']\n",
    "num_SD = config['ctgan_param']['num_sd']\n",
    "\n",
    "\n",
    "# run SDG generation\n",
    "# for each dataset specific settings\n",
    "for s_index, settings in enumerate(dataset_settings):\n",
    "    \n",
    "    if run_dataset is not None and settings['meta']['id'] not in run_dataset:\n",
    "        continue\n",
    "    \n",
    "    metadata = SingleTableMetadata().load_from_json(settings['meta']['meta_filepath'])\n",
    "    # Init experiment logging\n",
    "    experiment_name = f\"{settings['meta']['id']}-SDG\"\n",
    "    mlflow = MLFlowManager(experiment_name)\n",
    "    \n",
    "    # load original dataset\n",
    "    cols_dtype=None\n",
    "    if 'cols_dtype' in settings['meta']:\n",
    "        cols_dtyped = settings['meta']['cols_dtype']\n",
    "        \n",
    "    original_data = pd.read_csv(f\"{folders['real_dir']}{settings['meta']['filename']}\", dtype=cols_dtype)\n",
    "    \n",
    "    # get the size to generate the synthetic data\n",
    "    original_data_size = len(original_data)\n",
    "    sd_size = original_data_size * sd_size_factor\n",
    "    \n",
    "    logg_tags = {'Source': settings['meta']['id']}\n",
    "    \n",
    "    # loop through the different quality parameters for the SDG\n",
    "    for quality in quality_params:\n",
    "        \n",
    "        display(f\"Start: SDG-{settings['meta']['id']}{quality}\")\n",
    "        logg_tags['Quality'] = quality\n",
    "        \n",
    "        sdg_name = f\"S{settings['meta']['id']}{quality}\"\n",
    "        log_run = mlflow.start_run(sdg_name, tags=logg_tags)\n",
    "        # Get path to save the artifacts, relative to notebooks dir\n",
    "        artifact_path=log_run.info.artifact_uri.split('notebooks/')[1]\n",
    "        \n",
    "        mlflow.log_params(quality_params[quality])\n",
    "        \n",
    "        # creates model with sdg_param and quality_param as parameters\n",
    "        model = CTGAN(metadata=metadata, **quality_params[quality])\n",
    "        \n",
    "        if 'sdg_constraints' in settings['meta']:\n",
    "            model.add_constraints(constraints=settings['meta']['sdg_constraints'])\n",
    "        \n",
    "        print(\"#START#\")\n",
    "        print(sdg_name)\n",
    "        #model.fit(original_data)\n",
    "\n",
    "        model, stdout_loss = train_sdg_model(model, original_data, sdg_name)\n",
    "        # extract loss, create loss plot and save it\n",
    "        print(\"#END#\")\n",
    "        \n",
    "        loss_dict = extract_loss_info_from_stdout(stdout_loss)\n",
    "        fig = create_loss_plot(sdg_name, loss_dict[sdg_name])\n",
    "        \n",
    "        #save plot\n",
    "        \n",
    "        fig_path = f\"{artifact_path}/{sdg_name}_loss_plot.png\"\n",
    "        fig.savefig(fig_path)\n",
    "        #save data\n",
    "        \n",
    "        loss_df_path = f\"{artifact_path}/{sdg_name}_loss.csv\"\n",
    "        loss_dict[sdg_name].to_csv(loss_df_path, index=False)        \n",
    "        # saves the SDG model using cloudpickle\n",
    "        \n",
    "        model_path = f\"{artifact_path}/{sdg_name}.pkl\"\n",
    "        model.save(model_path)\n",
    "        #mlflow.end_run()\n",
    "        \n",
    "        # create num_SD SDGs and synthetic datasets for validating results\n",
    "        for itr in range(num_SD):\n",
    "            \n",
    "            # creates Synthetic dataset name, using datset id, quality key, and itr number \n",
    "            # e.g. SD1Q1_2 means SDG trained on datset D1 with quality Q1 and copy num 2\n",
    "            SD_name = f\"S{settings['meta']['id']}{quality}_{str(itr)}\"\n",
    "            \n",
    "            # relative file path for the synthetic dataset\n",
    "            sd_path = f\"{folders['sd_dir']}{SD_name}.csv\"\n",
    "            \n",
    "            # generate synthetic data\n",
    "            synthetic_data = model.sample(num_rows=sd_size)\n",
    "            \n",
    "            # save the synthetic dataset\n",
    "            synthetic_data.to_csv(sd_path, index=False)\n",
    "            \n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The Loss values captured from the cell above's standard output will \n",
    "be used to create the generator vs discriminator loss plots.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#Deprecated\n",
    "loss_values = extract_loss_info_from_stdout(stdout_loss.stdout)\n",
    "\n",
    "if (loss_values not None):\n",
    "    # Combine the loss values and save them\n",
    "    combined_loss_df = pd.concat(loss_values.values(), keys=loss_values.keys(), axis=0, ignore_index=False)\n",
    "    combined_loss_df = combined_loss_df.reset_index().rename(columns={'level_0': 'SDG'})\n",
    "    combined_loss_df.to_csv(f\"{folders['data_dir']}combined_sdg_loss.csv\", index=False)\n",
    "\n",
    "    for sdg_id in loss_values:\n",
    "        fig = create_loss_plot(sdg_id, loss_values[sdg_id])\n",
    "        # Save the plot to correct mlflow log\n",
    "        run=mlflow.load_run_by_name(sdg_id)\n",
    "        path=run.info.artifact_uri.replace(\"file:///\", \"\")\n",
    "        #save plot\n",
    "        fig.savefig(f\"{path}/{sdg_id}_loss_plot.png\")\n",
    "        #save data\n",
    "        loss_values[sdg_id].to_csv(f\"{path}/{sdg_id}.csv\", index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b96202-dbae-460d-8755-746e2629b1f1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 (syn)",
   "language": "python",
   "name": "syn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "163px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
