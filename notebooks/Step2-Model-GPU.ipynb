{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6cd288",
   "metadata": {},
   "source": [
    "# Step 4: Create models with SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00973f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (classification_report, \n\u001b[0;32m      8\u001b[0m                              roc_auc_score, \n\u001b[0;32m      9\u001b[0m                              matthews_corrcoef,\n\u001b[0;32m     10\u001b[0m                              cohen_kappa_score)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "source": [
    "# importing all packages needed in this section\n",
    "import os\n",
    "import sys \n",
    "import pandas\n",
    "import cudf as pd\n",
    "\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             roc_auc_score, \n",
    "                             matthews_corrcoef,\n",
    "                             cohen_kappa_score)\n",
    "\n",
    "from cuml.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "\n",
    "# utility functions for the experiment\n",
    "sys.path.append('../src')\n",
    "\n",
    "from mlflow_manager import MLFlowManager\n",
    "from tuning_grids import Grids\n",
    "from utils import getPicklesFromDir, getExperimentConfig, translate_model_name\n",
    "from gpuclassification import GPUClassifierPipeline, GPUModels, opt_tune_model\n",
    "\n",
    "# Get global experiment settings\n",
    "config = getExperimentConfig()\n",
    "folders = config['folders']\n",
    "# get a list of all settings for the datasets prepared beforehand\n",
    "dataset_settings = getPicklesFromDir(folders['settings_dir'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b291b",
   "metadata": {},
   "source": [
    "Create the dataset to save the performance. Initially was going to use mlflow for this. \n",
    "However, a bugg surfaced when google colab was used, where it got stuck in a endless loop\n",
    "trying to read the loggs via the colab cell. Thus this implementation.\n",
    "```\n",
    "Columns:\n",
    "    Dataset id: str\n",
    "        the dataset id that the model was evaluated on.\n",
    "    model: str\n",
    "        the shortend model name/id (e.g. lr = logistic regression, rf = random forest, etc.)\n",
    "    F1, Accuracy, AUC: float\n",
    "        performance metrics from evaluating the model on the hold-out data.\n",
    "    Params: dict\n",
    "        the hyperparameters for the model.\n",
    "    Tuned on: str\n",
    "        wheter the hyperparameters comes from tuning on original data or synthetic\n",
    "    Trained on: str\n",
    "        the type of data that the model was trained on, \"original\" or \"synthetic\"\n",
    "    Quality: str\n",
    "        if synthetic, the quality id of the generator\n",
    "    SDG:\n",
    "        the synthetic genenerator id.\n",
    "    Dataset type: str\n",
    "        if the dataset that the model trained on is \"original\" or \"synthetic\"\n",
    "    USI: str\n",
    "        Unique Settings Identifier, a unique string generated by pycaret setup each initialization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310233f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the dataset to save the performance. Initially was going to use mlflow for this. \n",
    "However, a bugg surfaced when google colab was used, where it got stuck in a endless loop\n",
    "trying to read the loggs via the colab cell. Thus this implementation.\n",
    "\n",
    "Columns:\n",
    "    Dataset id: str\n",
    "        the dataset id that the model was evaluated on.\n",
    "    model: str\n",
    "        the shortend model name/id (e.g. lr = logistic regression, rf = random forest, etc.)\n",
    "    F1, Accuracy, AUC: float\n",
    "        performance metrics from evaluating the model on the hold-out data.\n",
    "    Params: dict\n",
    "        the hyperparameters for the model.\n",
    "    Tuned on: str\n",
    "        wheter the hyperparameters comes from tuning on original data or synthetic\n",
    "    Trained on: str\n",
    "        the type of data that the model was trained on, \"original\" or \"synthetic\"\n",
    "    Quality: str\n",
    "        if synthetic, the quality id of the generator\n",
    "    SDG:\n",
    "        the synthetic genenerator id.\n",
    "    Dataset type: str\n",
    "        if the dataset that the model trained on is \"original\" or \"synthetic\"\n",
    "    USI: str\n",
    "        Unique Settings Identifier, a unique string generated by pycaret setup each initialization\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Create an empty DataFrame with the specified columns\n",
    "columns = [\"Dataset id\", \"model\", \"F1\", \"Accuracy\", \"AUC\", \"MCC\", \"Kappa\", \"Params\", \"Tuned on\", \"Trained on\", \"USI\", \"Quality\", \"SDG\"]\n",
    "\n",
    "# if it exists, read it, else create a new one\n",
    "if os.path.isfile(folders['model_perf_filepath']):\n",
    "    model_performance_df = pd.read_csv(folders['model_perf_filepath'])\n",
    "else:\n",
    "    model_performance_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "performance_row = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dataset = config['run_dataset']\n",
    "for settings in dataset_settings:\n",
    "        \n",
    "    if run_dataset is not None and settings['meta']['id'] not in run_dataset:\n",
    "        # Checks if run_dataset contains dataset_id's\n",
    "        # if it does, run the experiment only on specified datasets\n",
    "        continue\n",
    "    print(f\"Start models for {settings['meta']['id']}-{settings['meta']['name']}\")\n",
    "\n",
    "    dataset_path = f\"{folders['real_dir']}{settings['meta']['filename']}\"\n",
    "    target_label = settings['meta']['target']\n",
    "    train_size = settings['setup_param']['train_size']\n",
    "    settings['setup_param']['preprocess'] = False\n",
    "\n",
    "    #### Define features (use meta) ####\n",
    "    ordinal_features = settings['meta']['ordinal_features']\n",
    "    numeric_features = settings['meta']['numeric_features']\n",
    "    text_features = settings['meta']['text_features']\n",
    "    categorical_features = settings['meta']['categorical_features']\n",
    "\n",
    "    cols_dtype = None\n",
    "    if 'cols_dtype' in settings['meta']:\n",
    "      cols_dtype = settings['meta']['cols_dtype']\n",
    "    \n",
    "    # Load your dataset into a cuDF DataFrame\n",
    "    original_data = pd.read_csv(dataset_path, dtype=cols_dtype)\n",
    "\n",
    "    # Split the dataset into a train set and a test set using cuML's train_test_split function\n",
    "    print(\"Train test split ======\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X=original_data.drop(target_label, axis=1), \n",
    "        y=original_data[target_label], \n",
    "        train_size=train_size, \n",
    "        stratify=original_data[target_label], \n",
    "        shuffle=True)\n",
    "    \n",
    "    # Init experiment logging\n",
    "    experiment_name = f\"{settings['meta']['id']}-{settings['meta']['name']}\"\n",
    "    mlflow = MLFlowManager(experiment_name)\n",
    "    \n",
    "    logg_tags = {\n",
    "        'Dataset id': settings['meta']['id'],\n",
    "        'Tuned on': 'original',\n",
    "        'Trained on': 'original',\n",
    "    }\n",
    "    \n",
    "    mlflow.start_run(mlflow.run_name_with_original_data, tags=logg_tags)\n",
    "    \n",
    "    # for each defined model in the global config\n",
    "    # create specified model and tune it\n",
    "    for ml_model in config['clf']['ml_models']:\n",
    "        model_name = f\"{settings['meta']['id']}-{translate_model_name(ml_model)}\"\n",
    "\n",
    "        print(f\"Model: {translate_model_name(ml_model)}\")\n",
    "        print(\"Logg model name\")\n",
    "\n",
    "        logg_tags['model']=ml_model\n",
    "        print(\"start logging\")\n",
    "        \n",
    "        mlflow.start_run(model_name, tags=logg_tags, nested=True)\n",
    "        # create model and pipeline\n",
    "        print(\"Get GPU model\")\n",
    "        estimator = GPUModels(ml_model)\n",
    "        print(\"Create pipeline\")\n",
    "        model = GPUClassifierPipeline(\n",
    "            classifier=estimator,\n",
    "            numeric_features=numeric_features,\n",
    "            categorical_features=categorical_features,\n",
    "            ordinal_features=ordinal_features\n",
    "        )\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=config['clf']['cv_folds'])\n",
    "        optimize = config['clf']['tuning_param']['optimize']    \n",
    "        tune_grid = Grids.get_tuning_grid(ml_model, 'cuml')\n",
    "        \n",
    "        print(f\"Tune grid: {tune_grid}\")\n",
    "        \n",
    "        tuned_model, val_score = opt_tune_model(X=x_train, \n",
    "                                                y=y_train, \n",
    "                                                cv=cv, \n",
    "                                                model=model, \n",
    "                                                optimize=optimize, \n",
    "                                                tune_grid=tune_grid,\n",
    "                                                n_trials=config['clf']['tuning_param']['early_stopping_max_iters'])\n",
    "\n",
    "        y_pred = tuned_model.predict(x_test).to_pandas()\n",
    "\n",
    "        metrics =  classification_report(y_true=y_test.to_pandas(), y_pred=y_pred, output_dict=True, digits=4)\n",
    "        holdout_score = pandas.DataFrame.from_dict(metrics).transpose()\n",
    "\n",
    "        test_metrics = {\n",
    "            \"Accuracy\": metrics['accuracy'],\n",
    "            \"F1\": metrics['macro avg']['f1-score'],\n",
    "            \"MCC\": matthews_corrcoef(y_true=y_test.to_pandas(), y_pred=y_pred),\n",
    "            \"Kappa\": cohen_kappa_score(y1=y_test.to_pandas(), y2=y_pred)\n",
    "        }\n",
    "\n",
    "        # If there is a prediction_score in the from predict_model (sometimes there isn't)\n",
    "        if y_test.nunique() == 2:\n",
    "            y_prob = tuned_model.predict_proba(x_test)\n",
    "            test_metrics['AUC'] = roc_auc_score(y_true=y_test.to_pandas, y_score=y_prob)\n",
    "        \n",
    "        # log parameters     \n",
    "        mlflow.log_params(tuned_model.get_classifier().get_params())\n",
    "        # log performance\n",
    "        mlflow.log_tag('model', ml_model)\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "        mlflow.log_metric(f\"val_{optimize}\", val_score)\n",
    "        mlflow.log_score_report_to_html(holdout_score, \"Holdout\")\n",
    "        # log model\n",
    "        mlflow.log_model(model=tuned_model)\n",
    "        # end run for the model\n",
    "        mlflow.end_run()\n",
    "        \n",
    "        # quick fix for colab issue\n",
    "        performance_row = {**logg_tags, **test_metrics}\n",
    "        performance_row['Params'] = tuned_model.get_params()\n",
    "        model_performance_df = model_performance_df.append(performance_row, ignore_index=True)\n",
    "\n",
    "    # end run for this dataset\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Save model performance to csv\n",
    "model_performance_df.to_csv(folders['model_perf_filepath'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "163px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
